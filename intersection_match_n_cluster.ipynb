{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from fuzzywuzzy import fuzz\n",
    "import requests\n",
    "from sklearn.cluster import DBSCAN\n",
    "import geopandas as gpd\n",
    "from shapely import Point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the City of Vancouver old and recent scraped TMC data from 1989 to 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_tmc_df = pd.read_csv(os.path.join('data','scraped','old_tmc_scraped.csv'))\n",
    "recent_tmc_df = pd.read_csv(os.path.join('data','scraped','recent_tmc_scraped.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brief EDA and Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2521 entries, 0 to 2520\n",
      "Data columns (total 51 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   date                2507 non-null   object \n",
      " 1   intersection        2521 non-null   object \n",
      " 2   longitude           2521 non-null   float64\n",
      " 3   latitude            2521 non-null   float64\n",
      " 4   weather             2477 non-null   object \n",
      " 5   type                2507 non-null   object \n",
      " 6   AM_available        2507 non-null   object \n",
      " 7   AM_scraped          2507 non-null   object \n",
      " 8   MD_available        2507 non-null   object \n",
      " 9   MD_scraped          2507 non-null   object \n",
      " 10  PM_available        2507 non-null   object \n",
      " 11  PM_scraped          2507 non-null   object \n",
      " 12  AM_peak_hour        2373 non-null   object \n",
      " 13  AM_north_bikes_vol  2373 non-null   float64\n",
      " 14  AM_north_peds_vol   2373 non-null   float64\n",
      " 15  AM_north_veh_vol    2295 non-null   float64\n",
      " 16  AM_east_bikes_vol   2373 non-null   float64\n",
      " 17  AM_east_peds_vol    2373 non-null   float64\n",
      " 18  AM_east_veh_vol     2297 non-null   float64\n",
      " 19  AM_south_bikes_vol  2282 non-null   float64\n",
      " 20  AM_south_peds_vol   2373 non-null   float64\n",
      " 21  AM_south_veh_vol    2281 non-null   float64\n",
      " 22  AM_west_bikes_vol   2373 non-null   float64\n",
      " 23  AM_west_peds_vol    2373 non-null   float64\n",
      " 24  AM_west_veh_vol     2319 non-null   float64\n",
      " 25  PM_peak_hour        2098 non-null   object \n",
      " 26  PM_north_bikes_vol  2098 non-null   float64\n",
      " 27  PM_north_peds_vol   2098 non-null   float64\n",
      " 28  PM_north_veh_vol    2029 non-null   float64\n",
      " 29  PM_east_bikes_vol   2098 non-null   float64\n",
      " 30  PM_east_peds_vol    2098 non-null   float64\n",
      " 31  PM_east_veh_vol     2028 non-null   float64\n",
      " 32  PM_south_bikes_vol  2012 non-null   float64\n",
      " 33  PM_south_peds_vol   2015 non-null   float64\n",
      " 34  PM_south_veh_vol    2012 non-null   float64\n",
      " 35  PM_west_bikes_vol   2098 non-null   float64\n",
      " 36  PM_west_peds_vol    2098 non-null   float64\n",
      " 37  PM_west_veh_vol     2051 non-null   float64\n",
      " 38  MD_peak_hour        418 non-null    object \n",
      " 39  MD_north_bikes_vol  418 non-null    float64\n",
      " 40  MD_north_peds_vol   418 non-null    float64\n",
      " 41  MD_north_veh_vol    401 non-null    float64\n",
      " 42  MD_east_bikes_vol   418 non-null    float64\n",
      " 43  MD_east_peds_vol    418 non-null    float64\n",
      " 44  MD_east_veh_vol     403 non-null    float64\n",
      " 45  MD_south_bikes_vol  405 non-null    float64\n",
      " 46  MD_south_peds_vol   406 non-null    float64\n",
      " 47  MD_south_veh_vol    405 non-null    float64\n",
      " 48  MD_west_bikes_vol   418 non-null    float64\n",
      " 49  MD_west_peds_vol    418 non-null    float64\n",
      " 50  MD_west_veh_vol     407 non-null    float64\n",
      "dtypes: float64(38), object(13)\n",
      "memory usage: 1004.6+ KB\n",
      "Number of missing counts: 14\n"
     ]
    }
   ],
   "source": [
    "old_tmc_df.info()\n",
    "print(f\"Number of missing counts: {old_tmc_df['date'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The old tmc data from 1989 to 2011 contains 2521 counts, from which 14 counts were not scraped due to an abnoraml report format. These 14 counts are removed from the analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_tmc_df = old_tmc_df[~old_tmc_df['date'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1796 entries, 0 to 1795\n",
      "Data columns (total 49 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   date                1782 non-null   object \n",
      " 1   intersection        1796 non-null   object \n",
      " 2   weather             1790 non-null   object \n",
      " 3   type                294 non-null    object \n",
      " 4   AM_available        1791 non-null   object \n",
      " 5   AM_scraped          1791 non-null   object \n",
      " 6   MD_available        1791 non-null   object \n",
      " 7   MD_scraped          1791 non-null   object \n",
      " 8   PM_available        1791 non-null   object \n",
      " 9   PM_scraped          1791 non-null   object \n",
      " 10  AM_peak_hour        1770 non-null   object \n",
      " 11  AM_north_bikes_vol  1263 non-null   float64\n",
      " 12  AM_north_peds_vol   1651 non-null   float64\n",
      " 13  AM_north_veh_vol    1592 non-null   float64\n",
      " 14  AM_east_bikes_vol   1328 non-null   float64\n",
      " 15  AM_east_peds_vol    1664 non-null   float64\n",
      " 16  AM_east_veh_vol     1663 non-null   float64\n",
      " 17  AM_south_bikes_vol  1360 non-null   float64\n",
      " 18  AM_south_peds_vol   1656 non-null   float64\n",
      " 19  AM_south_veh_vol    1588 non-null   float64\n",
      " 20  AM_west_bikes_vol   1283 non-null   float64\n",
      " 21  AM_west_peds_vol    1671 non-null   float64\n",
      " 22  AM_west_veh_vol     1631 non-null   float64\n",
      " 23  PM_peak_hour        1748 non-null   object \n",
      " 24  PM_north_bikes_vol  1371 non-null   float64\n",
      " 25  PM_north_peds_vol   1630 non-null   float64\n",
      " 26  PM_north_veh_vol    1570 non-null   float64\n",
      " 27  PM_east_bikes_vol   1346 non-null   float64\n",
      " 28  PM_east_peds_vol    1629 non-null   float64\n",
      " 29  PM_east_veh_vol     1650 non-null   float64\n",
      " 30  PM_south_bikes_vol  1351 non-null   float64\n",
      " 31  PM_south_peds_vol   1639 non-null   float64\n",
      " 32  PM_south_veh_vol    1557 non-null   float64\n",
      " 33  PM_west_bikes_vol   1366 non-null   float64\n",
      " 34  PM_west_peds_vol    1638 non-null   float64\n",
      " 35  PM_west_veh_vol     1628 non-null   float64\n",
      " 36  MD_peak_hour        399 non-null    object \n",
      " 37  MD_north_bikes_vol  261 non-null    float64\n",
      " 38  MD_north_peds_vol   350 non-null    float64\n",
      " 39  MD_north_veh_vol    338 non-null    float64\n",
      " 40  MD_east_bikes_vol   276 non-null    float64\n",
      " 41  MD_east_peds_vol    347 non-null    float64\n",
      " 42  MD_east_veh_vol     376 non-null    float64\n",
      " 43  MD_south_bikes_vol  277 non-null    float64\n",
      " 44  MD_south_peds_vol   344 non-null    float64\n",
      " 45  MD_south_veh_vol    353 non-null    float64\n",
      " 46  MD_west_bikes_vol   270 non-null    float64\n",
      " 47  MD_west_peds_vol    346 non-null    float64\n",
      " 48  MD_west_veh_vol     361 non-null    float64\n",
      "dtypes: float64(36), object(13)\n",
      "memory usage: 687.7+ KB\n",
      "Number of missing counts: 14\n"
     ]
    }
   ],
   "source": [
    "recent_tmc_df.info()\n",
    "print(f\"Number of missing counts: {recent_tmc_df['date'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The recent tmc data from 2012 to 2022 contains 1796 counts, from which 14 counts were not scraped due to an abnoraml report format. These 14 counts are removed from the analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "recent_tmc_df = recent_tmc_df[~recent_tmc_df['date'].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The old tmc data contains the latitude and longitude of the intersections, while the recent tmc data doesn't. In the next step, we can try using a fuzzy match \n",
    "to populate the latitude and longitude data for the recemt_tmc_df using the intersection name. We could also perfrom geocoding using an API but I leave that for someone else which is probably an easier way to do. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'latitude', 'longitude'}"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(old_tmc_df.columns) - set(recent_tmc_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Intersection name fuzzy match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perfrom a fuzzy match, we need to divide the intersection names to primary_rd and secondary_rd to enable a precise search independent of the street names order across intersection names. The first step is to replace the '&' with 'AND' to enable seperating the intersection name to primary_rd and secondary_rd using 'AND' across all intersection names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recent_tmc_df[recent_tmc_df['intersection'].apply(lambda x: '&' in x )].head()\n",
    "# there are some \"&\" present in the recent_tmc_df which we can replace with \"AND\" for consistency.\n",
    "name_filter = recent_tmc_df['intersection'].apply(lambda x: '&' in x )\n",
    "recent_tmc_df.loc[name_filter, 'intersection'] = recent_tmc_df.loc[name_filter, 'intersection'].apply(lambda x: x.replace('&', 'AND'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to load the City of Vancouver intersection data that contains the latitude and longitude of each intersection in addition to its name.\n",
    "\n",
    "#NOTE: instead of using the intersection csv file from the City of Vancouver to condutct a fuzzy match, someone can try fetching the coordinates one by one using the City of Vancouver's API. However, that won't be any faster and you'd need to call the API once for the intersection name as is and and once for the flipped name of the intersection to ensure the order of intersection appraoches' name is accounted for. The commented out block below is the code for the later approach if you are curious:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# import pandas as pd\n",
    "\n",
    "# def get_intersection_coordinates(intersection_name):\n",
    "\n",
    "#     encoded_name = requests.utils.quote(intersection_name)\n",
    "    \n",
    "#     url = f\"https://opendata.vancouver.ca/api/explore/v2.1/catalog/datasets/street-intersections/records?where=xstreet='{encoded_name}'&limit=1\"\n",
    "    \n",
    "#     response = requests.get(url)\n",
    "#     if response.status_code == 200:\n",
    "#         data = response.json()\n",
    "#         if data['total_count'] > 0:\n",
    "#             result = data['results'][0]\n",
    "#             coordinates = result['geo_point_2d']\n",
    "#             return coordinates\n",
    "#         else:\n",
    "#             return \"Intersection not found\"\n",
    "#     else:\n",
    "#         return f\"Failed to retrieve data: {response.status_code}\"\n",
    "    \n",
    "\n",
    "# df = pd.DataFrame({\n",
    "#     'Intersection': ['GRANVILLE ST AND MARPOLE AV', 'MARPOLE AV AND GRANVILLE ST']\n",
    "# })\n",
    "\n",
    "# def add_coordinates_to_df(df, column_name):\n",
    "#     df['Coordinates'] = df[column_name].apply(get_intersection_coordinates)\n",
    "#     return df\n",
    "\n",
    "# df_with_coordinates = add_coordinates_to_df(df, 'Intersection')\n",
    "\n",
    "# print(df_with_coordinates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "intersections = pd.read_csv(os.path.join('data','raw','street-intersections-2.csv'),sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below find_best_intersection_match function aims to find the most similar intersection name from a list of intersection names compared to a given  intersection name which in our case is the recent tmc count intersection names. It does this using fuzzy string matching, specifically the fuzz.partial_ratio method, which compares parts of strings for similarity. The recent intersection name is split into primary and secondary road names, and each is individually compared against each part of every intersection name. The best match scores for primary and secondary parts are determined, and the overall score for an old intersection is the minimum of these two best scores. If this overall score is greater than any previously found score and meets or exceeds a 100% score cutoff, that intersection is considered the best match so far. The function ultimately returns the intersection name with the highest score that also meets or exceeds the cutoff, ensuring an exact or nearly exact match due to the 100% threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_intersection_match(recent_intersection_name, intersection_names, score_cutoff):\n",
    "    best_score = 0\n",
    "    best_match = None\n",
    "\n",
    "    primary_rd = recent_intersection_name.split(' AND ')[0].strip().upper()\n",
    "    secondary_rd = recent_intersection_name.split(' AND ')[1].strip().upper()\n",
    "    \n",
    "    for intersection in intersection_names:\n",
    "\n",
    "        intersection_parts = [part.strip() for part in intersection.upper().split(' AND ')]\n",
    "\n",
    "        primary_best_score = 0\n",
    "        secondary_best_score = 0\n",
    "\n",
    "        for part in intersection_parts:\n",
    "            primary_score = fuzz.partial_ratio(primary_rd, part)\n",
    "            secondary_score = fuzz.partial_ratio(secondary_rd, part)\n",
    "            \n",
    "            if primary_score > primary_best_score:\n",
    "                primary_best_score = primary_score\n",
    "            if secondary_score > secondary_best_score:\n",
    "                secondary_best_score = secondary_score\n",
    "\n",
    "                \n",
    "        overall_score = min(primary_best_score, secondary_best_score)\n",
    "        \n",
    "        if overall_score > best_score and overall_score >= score_cutoff:\n",
    "            best_score = overall_score\n",
    "            best_match = intersection\n",
    "\n",
    "    return best_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "matching_threshold = 100\n",
    "recent_tmc_df['matched_intersection_name'] = recent_tmc_df.apply(lambda x: find_best_intersection_match(x['intersection'], intersections['XSTREET'], score_cutoff=matching_threshold), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After applying the function, 1636 intersections were matched and 146 intersections were left unmatched. In the next step we merge the recent tmc count data and the intersection data on the matched intersection name to populate the coordinates for the matched intersections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched intersections: 1636\n",
      "Unmatched intersections: 146\n"
     ]
    }
   ],
   "source": [
    "print(f\"Matched intersections: {recent_tmc_df['matched_intersection_name'].notna().sum()}\")\n",
    "print(f\"Unmatched intersections: {recent_tmc_df['matched_intersection_name'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "recent_tmc_df = pd.merge(left=recent_tmc_df,right=intersections.drop_duplicates(subset=['XSTREET'])[['XSTREET','geo_point_2d']],left_on='matched_intersection_name',right_on='XSTREET',how='left')\n",
    "recent_tmc_df[['latitude','longitude']] = recent_tmc_df['geo_point_2d'].str.split(',',expand=True)\n",
    "recent_tmc_df.drop(columns=['XSTREET','geo_point_2d'],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step, we can try using the fuzzy match once more but now only for the unmatched intersections and using the older counts data that we have the coordinates for.Just one more step before giving up to a geocoder! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "unmathced_recent_tmc_df = pd.DataFrame()\n",
    "unmathced_recent_tmc_df = recent_tmc_df[recent_tmc_df['matched_intersection_name'].isna()]\n",
    "unmathced_recent_tmc_df = unmathced_recent_tmc_df.drop(columns=['matched_intersection_name','latitude','longitude'])\n",
    "recent_tmc_df = recent_tmc_df.dropna(subset=['matched_intersection_name']).reset_index(drop=True)\n",
    "recent_tmc_df.drop(columns=['matched_intersection_name'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "matching_threshold = 100\n",
    "unmathced_recent_tmc_df['matched_intersection_name'] = unmathced_recent_tmc_df.apply(lambda x: find_best_intersection_match(x['intersection'], old_tmc_df['intersection'], score_cutoff=matching_threshold), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We were able to martch 17 more intersections. However, the remaining intersections probably have more than two approaches' name and that's why we couldn't match them with of the data sources at my our disposal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched intersections: 17\n",
      "Unmatched intersections: 129\n"
     ]
    }
   ],
   "source": [
    "print(f\"Matched intersections: {unmathced_recent_tmc_df['matched_intersection_name'].notna().sum()}\")\n",
    "print(f\"Unmatched intersections: {unmathced_recent_tmc_df['matched_intersection_name'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "unmathced_recent_tmc_df = pd.merge(left=unmathced_recent_tmc_df,right=old_tmc_df.drop_duplicates(subset=['intersection'])[['intersection','latitude','longitude']]\n",
    "                            ,how='left',left_on=['matched_intersection_name'],right_on=['intersection'])\n",
    "unmathced_recent_tmc_df.drop(columns=['intersection_y','matched_intersection_name'],inplace=True)\n",
    "unmathced_recent_tmc_df.rename(columns={'intersection_x':'intersection'},inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step we concat back the newly matched intersection to our recent tmc data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "recent_tmc_df = pd.concat([recent_tmc_df,unmathced_recent_tmc_df[unmathced_recent_tmc_df['latitude'].notna()]]).reset_index(drop=True)\n",
    "unmathced_recent_tmc_df = unmathced_recent_tmc_df[unmathced_recent_tmc_df['latitude'].isna()].reset_index(drop=True)\n",
    "unmathced_recent_tmc_df.drop(columns=['latitude','longitude'],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the reaming 129 intersections, we can use the the government of BC geocoder. From 129 intersections, some of them are duplicated intersection which we can drop for the geocing purposes now that we are using an api. We could have done this previouly as well to speed the process, but that wasn't the goal. However, for the api use, we should especially if there is a cap for the number of free geocodings and the speed of geocoding is limited.\n",
    "\n",
    "Ref: https://www2.gov.bc.ca/gov/content/data/geographic-data-services/location-services/geocoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "addresses = pd.DataFrame(unmathced_recent_tmc_df.drop_duplicates(subset=['intersection'])['intersection']+ ', Vancouver').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geocode_address(address):\n",
    "    url = 'https://geocoder.api.gov.bc.ca/addresses.json'\n",
    "    params = {\n",
    "        'addressString': address,\n",
    "        'locationDescriptor': 'any',\n",
    "        'maxResults': 1,\n",
    "        'interpolation': 'adaptive',\n",
    "        'echo': True,\n",
    "        'brief': False,\n",
    "        'autoComplete': False,\n",
    "        'setBack': 0,\n",
    "        'outputSRS': 4326,\n",
    "        'minScore': 1,\n",
    "        'provinceCode': 'BC'\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, params=params)\n",
    "        response.raise_for_status()  \n",
    "        data = response.json()\n",
    "        if data.get(\"features\"):\n",
    "            coordinates = data.get(\"features\")[0].get('geometry').get('coordinates')\n",
    "            return coordinates  # return as [longitude, latitude]\n",
    "        else:\n",
    "            return [None, None]  \n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Request failed: {e}\")\n",
    "        return [None, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "addresses['coordinates'] = addresses['intersection'].apply(geocode_address)\n",
    "addresses[['longitude', 'latitude']] = pd.DataFrame(addresses['coordinates'].tolist(), index=addresses.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The geocoder was able to match a coordiante for every address. However, some of the addresses are not matched correctly and could be adjusted manually which is beyond the scope of this kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "addresses.drop(columns=['coordinates'],inplace=True)\n",
    "addresses['intersection'] = addresses['intersection'].str.split(',',expand=True)[0]\n",
    "unmathced_recent_tmc_df = pd.merge(unmathced_recent_tmc_df,addresses,on='intersection',how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the last step before concating the recent and old tmc count, we can add the unmatched tmc counts back to our recent tmc count dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "recent_tmc_df = pd.concat([recent_tmc_df,unmathced_recent_tmc_df]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmc = pd.concat([recent_tmc_df,old_tmc_df]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the intersections at the same location may have variable names, we need to conduct a geospationl clustering to match them together. This can be done using a DBSCAN clustering to generate intersection id's for the intersection that may have different name but they are actually refering to the same location. *Note: Beware of intersections where there is an overpass and underpass with the same coordiante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "coordinates = tmc[['latitude', 'longitude']].to_numpy()\n",
    "rad_coordinates = np.radians(coordinates)\n",
    "db = DBSCAN(eps=0.0000000005, min_samples=1, metric='haversine').fit(rad_coordinates)\n",
    "tmc.insert(2,'intersection_id',db.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_names = tmc.groupby('intersection_id')['intersection'].first().reset_index()\n",
    "tmc = pd.merge(tmc.drop('intersection', axis=1), aggregated_names, on='intersection_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmc = gpd.GeoDataFrame(\n",
    "    tmc,\n",
    "    geometry=[Point(xy) for xy in zip(tmc.longitude, tmc.latitude)],crs=\"EPSG:4326\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmc.to_csv('tmc_viz.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
